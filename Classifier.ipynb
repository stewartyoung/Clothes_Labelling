{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MINST Item Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Goal: Use the brightness levels of pixels in the image dataset to classify types of fashion items.\n",
    "\n",
    "First we'll need to load in the dataset and see what it looks like\n",
    "\n",
    "## Dataset Loader and Visualiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28) y_train shape: (60000,)\n",
      "x_test shape: (10000, 28, 28) y_test shape: (10000,)\n",
      "205\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape, \"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Show one of the images from the training dataset\n",
    "plt.imshow(x_train[27])\n",
    "print(x_train[27][15][15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training data further into 80% training and 20% validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_valid shape: (12000, 28, 28) y_valid shape: (12000,)\n",
      "x_train shape: (48000, 28, 28) y_train shape: (48000,)\n",
      "x_test shape: (10000, 28, 28) y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2)\n",
    "\n",
    "print(\"x_valid shape:\", x_valid.shape, \"y_valid shape:\", y_valid.shape)\n",
    "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape, \"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_valid shape: (12000, 28, 28) y_valid shape: (12000,)\n",
      "x_train shape: (48000, 28, 28) y_train shape: (48000,)\n",
      "x_test shape: (10000, 28, 28) y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, (5, 5), padding=\"same\", input_shape=[28, 28, 1]),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "    keras.layers.Conv2D(64, (5, 5), padding=\"same\"),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1024, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# model = tf.keras.Sequential()\n",
    "\n",
    "print(\"x_valid shape:\", x_valid.shape, \"y_valid shape:\", y_valid.shape)\n",
    "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape, \"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Take a look at the model summary\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_5_input to have 4 dimensions, but got array with shape (48000, 28, 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-bdafd31c80f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m          \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m          \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m          \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m          )\n",
      "\u001b[1;32mC:\\Miniconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1276\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'steps_per_epoch'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m         validation_split=validation_split)\n\u001b[0m\u001b[0;32m   1279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[0;32m    876\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 878\u001b[1;33m         exception_prefix='input')\n\u001b[0m\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    180\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_5_input to have 4 dimensions, but got array with shape (48000, 28, 28)"
     ]
    }
   ],
   "source": [
    "# checkpoint = ModelCheckpoint(filepath='/Users/stewa/checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "model.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=64,\n",
    "         epochs=10,\n",
    "         validation_data=(x_valid, y_valid)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
